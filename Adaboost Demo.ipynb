{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAFgCAYAAACmDI9oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElqGJAlSZKkhgFZkiRJahiQJUmSpMYLMY0UnNgCLj4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa302eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import graphviz \n",
    "# Copied from 'sklearn two class Adaboost'\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Noel Dawe <noel.dawe@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=4, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=6, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=5)\n",
    "\n",
    "bdt.fit(X, y)\n",
    "\n",
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.subplot(121)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the training points\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1],\n",
    "                c=c, cmap=plt.cm.Paired,\n",
    "                s=20, edgecolor='k',\n",
    "                label=\"Class %s\" % n)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Decision Boundary')\n",
    "\n",
    "# Plot the two-class decision scores\n",
    "twoclass_output = bdt.decision_function(X)\n",
    "plot_range = (twoclass_output.min(), twoclass_output.max())\n",
    "plt.subplot(122)\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    plt.hist(twoclass_output[y == i],\n",
    "             bins=10,\n",
    "             range=plot_range,\n",
    "             facecolor=c,\n",
    "             label='Class %s' % n,\n",
    "             alpha=.5,\n",
    "             edgecolor='k')\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1, y2 * 1.2))\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Samples')\n",
    "plt.xlabel('Score')\n",
    "plt.title('Decision Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since I am new to boosting, would like to visualize the process of the additive of weak learners.\n",
    "# Will use 'two class adaboost' as an example to analyze the boosting process step by step\n",
    "# Here is the Algorithm of AdaBoost....\n",
    "# Initialize the observation weights Wi = 1/n, i = 1,2,...,n.\n",
    "# For m = 1 to M:\n",
    "#   (a) Fit a classifier Tm(x) to the training data using weights Wi.\n",
    "#   (b) Compute error term :err(m) = sum(Wi*I(ci!=Tm(xi)))/sum(Wi)\n",
    "#   (c) Compute alpha(m) = log((1-err(m))/err(m))\n",
    "#   (d) Set Wi <-- Wi*exp(alpha(m)*I(ci!=Tm(xi))), i=1,2,...,n. Set a larger weight for the previously misclassified\n",
    "#   observations\n",
    "#   (e) renormalize Wi = Wi/sum(Wi)\n",
    "#   Output C(x)= argmax(sumover m (alpha(m)*I(Tm(xi)=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.29717124, -0.86515422],\n",
       "       [ 2.46753646, -1.07650912],\n",
       "       [-0.74694766, -1.51740678],\n",
       "       [ 1.22387121, -3.25486724],\n",
       "       [ 4.98940865,  2.25075447],\n",
       "       [ 4.7907092 ,  0.47685323],\n",
       "       [ 5.13694926,  2.06771575],\n",
       "       [ 4.05990356,  0.18120228],\n",
       "       [ 2.35312436,  1.68588718],\n",
       "       [ 3.3907415 ,  2.69458491]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X has 2 features X0,X1\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize observation weight\n",
    "weight0 = np.array([0.1]*10)\n",
    "weight0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=1025899737, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=1105291243, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=1371072018, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=1649962599, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=74382194, splitter='best')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaboost has an attribute 'estimators_' by which you can access each decision tree and visualize them\n",
    "bdt.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"268pt\" height=\"161pt\"\r\n",
       " viewBox=\"0.00 0.00 268.00 161.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 157)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-157 264,-157 264,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178,-153C178,-153 81,-153 81,-153 75,-153 69,-147 69,-141 69,-141 69,-101 69,-101 69,-95 75,-89 81,-89 81,-89 178,-89 178,-89 184,-89 190,-95 190,-101 190,-101 190,-141 190,-141 190,-147 184,-153 178,-153\"/>\r\n",
       "<text text-anchor=\"start\" x=\"90.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\r\n",
       "<text text-anchor=\"start\" x=\"100.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\r\n",
       "<text text-anchor=\"start\" x=\"107.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 2.1592</text>\r\n",
       "<text text-anchor=\"start\" x=\"100.5\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"86\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"start\" x=\"77\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.5, 0.5]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.400000\" stroke=\"black\" d=\"M109,-53C109,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 109,-0 109,-0 115,-0 121,-6 121,-12 121,-12 121,-41 121,-41 121,-47 115,-53 109,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4687</text>\r\n",
       "<text text-anchor=\"start\" x=\"21\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.5, 0.3]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.332,-88.9415C99.6776,-80.0207 92.3905,-70.2517 85.624,-61.1807\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"88.3741,-59.0137 79.5894,-53.0908 82.7631,-63.1992 88.3741,-59.0137\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"75.9895\" y=\"-74.1303\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M248,-53C248,-53 151,-53 151,-53 145,-53 139,-47 139,-41 139,-41 139,-12 139,-12 139,-6 145,-0 151,-0 151,-0 248,-0 248,-0 254,-0 260,-6 260,-12 260,-12 260,-41 260,-41 260,-47 254,-53 248,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"170.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"160\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\r\n",
       "<text text-anchor=\"start\" x=\"147\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.0, 0.2]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.004,-88.9415C159.755,-80.0207 167.147,-70.2517 174.012,-61.1807\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.89,-63.177 180.134,-53.0908 171.309,-58.9529 176.89,-63.177\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"183.563\" y=\"-74.1546\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x4bdf828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First decision tree\n",
    "dot_data = tree.export_graphviz(bdt.estimators_[0], out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below observations are belong to the left subnode:\n",
      "(array([0, 1, 2, 3, 5, 6, 7, 8], dtype=int64),)\n",
      "\n",
      "The classfication of these observations are:\n",
      "[0 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# First Decision Tree use X1<= 2.1592 as split criteria. gini is calculated as 1-p^2-q^2. So the gini for root is\n",
    "# 1-0.5^2-0.5^2=0.5(p and q are the success and failure ratio and in this case are both 0.5 since we have balanced dataset)\n",
    "# the initial weight for all samples are 1/n = 0.1 (n=10), so root value = [0.5,0.5]\n",
    "# Now let's look at the subnodes after the split. Firstly Let's check the left subnode with gini=0.4687\n",
    "print 'The below observations are belong to the left subnode:'\n",
    "print np.where(X[:,1]<=2.1592)\n",
    "print\n",
    "print 'The classfication of these observations are:'\n",
    "print y[X[:,1]<=2.1592]\n",
    "# As we can see below, there are 8 observations in this subnode; There are 5 observations are with class 0 and\n",
    "# 3 observations are with class 1 so the weight value = [0.5,0.3] indicating 3 observations got misclassified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below observations are belong to the right subnode:\n",
      "(array([4, 9], dtype=int64),)\n",
      "\n",
      "The classification of these observations are:\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# Next Let's look at the right subnode.\n",
    "print 'The below observations are belong to the right subnode:'\n",
    "print np.where(~(X[:,1]<=2.1592))\n",
    "print\n",
    "print 'The classification of these observations are:'\n",
    "print y[X[:,1]>2.1592]\n",
    "# There are 2 observations in this subnonde; both of them are labeled class 1. The weight value = [0.0,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the misclassified observations are:\n",
      "[1, 3, 8]\n",
      "the weight of these observations are:\n",
      "[0.1, 0.1, 0.1]\n",
      "The first error term err0 is: 0.3\n",
      "\n",
      "The first alpha term alpla0 is: 0.847297860387\n"
     ]
    }
   ],
   "source": [
    "# the error term of the first decision tree is\n",
    "print 'the misclassified observations are:'\n",
    "print [1,3,8]\n",
    "print 'the weight of these observations are:'\n",
    "print [0.1,0.1,0.1]\n",
    "misclassified = np.array([0.1,0.1,0.1])\n",
    "err0 = misclassified.sum()/weight0.sum()\n",
    "print 'The first error term err0 is:', err0\n",
    "print\n",
    "alpha0 = np.log((1-err0)/err0)\n",
    "print 'The first alpha term alpla0 is:',alpha0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learner weight of first Decision Tree is: 0.847297860387\n",
      "The error of first Decision Tree is: 0.3\n"
     ]
    }
   ],
   "source": [
    "# We can also cross verify the alpha0 value using method estimators_weight_\n",
    "print 'The learner weight of first Decision Tree is:', bdt.estimator_weights_[0]\n",
    "print 'The error of first Decision Tree is:', bdt.estimator_errors_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample weight after first round is:\n",
      "[ 0.07142857  0.16666667  0.07142857  0.16666667  0.07142857  0.07142857\n",
      "  0.07142857  0.07142857  0.16666667  0.07142857]\n"
     ]
    }
   ],
   "source": [
    "# Now We need to update and renomalize the weight of all the observations, \n",
    "# Misclassified observations will be boosted in weights. \n",
    "print 'The sample weight after first round is:'\n",
    "weight1 = np.copy(weight0)\n",
    "weight1[[1,3,8]]=weight0[1]*np.exp(alpha0)\n",
    "weight1 = weight1/weight1.sum()\n",
    "print weight1\n",
    "# It can be seen that observation 1,3,8 has been boosted in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first stage decision function is:\n",
      "[-1. -1. -1. -1.  1. -1. -1. -1. -1.  1.]\n",
      "The first predict is:\n",
      "[0 0 0 0 1 0 0 0 0 1]\n",
      "The first predict probability is:\n",
      "[[ 0.5621765   0.4378235 ]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.26894142  0.73105858]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.5621765   0.4378235 ]\n",
      " [ 0.26894142  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "# The first stage decision function\n",
    "print 'The first stage decision function is:'\n",
    "for item in bdt.staged_decision_function(X):\n",
    "    print item\n",
    "    break\n",
    "# The first predict\n",
    "print 'The first predict is:'\n",
    "for item in bdt.staged_predict(X):\n",
    "    print item\n",
    "    predict0=item\n",
    "    break\n",
    "# The first predict probability\n",
    "print 'The first predict probability is:'\n",
    "for item in bdt.staged_predict_proba(X):\n",
    "    print item\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"366pt\" height=\"161pt\"\r\n",
       " viewBox=\"0.00 0.00 366.00 161.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 157)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-157 362,-157 362,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.443137\" stroke=\"black\" d=\"M252,-153C252,-153 106,-153 106,-153 100,-153 94,-147 94,-141 94,-141 94,-101 94,-101 94,-95 100,-89 106,-89 106,-89 252,-89 252,-89 258,-89 264,-95 264,-101 264,-101 264,-141 264,-141 264,-147 258,-153 252,-153\"/>\r\n",
       "<text text-anchor=\"start\" x=\"140\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\r\n",
       "<text text-anchor=\"start\" x=\"150\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\r\n",
       "<text text-anchor=\"start\" x=\"157\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 3.7253</text>\r\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4592</text>\r\n",
       "<text text-anchor=\"start\" x=\"135.5\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"start\" x=\"102\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.3571, 0.6429]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.749020\" stroke=\"black\" d=\"M158,-53C158,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 158,-0 158,-0 164,-0 170,-6 170,-12 170,-12 170,-41 170,-41 170,-47 164,-53 158,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"51.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.32</text>\r\n",
       "<text text-anchor=\"start\" x=\"45.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.1429, 0.5714]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147.438,-88.9415C138.095,-79.7477 127.836,-69.6528 118.382,-60.3497\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.588,-57.6101 111.006,-53.0908 115.679,-62.5995 120.588,-57.6101\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"111.207\" y=\"-74.39\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.666667\" stroke=\"black\" d=\"M346,-53C346,-53 200,-53 200,-53 194,-53 188,-47 188,-41 188,-41 188,-12 188,-12 188,-6 194,-0 200,-0 200,-0 346,-0 346,-0 352,-0 358,-6 358,-12 358,-12 358,-41 358,-41 358,-47 352,-53 346,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"235.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.375</text>\r\n",
       "<text text-anchor=\"start\" x=\"233.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\r\n",
       "<text text-anchor=\"start\" x=\"196\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.2143, 0.0714]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.562,-88.9415C219.905,-79.7477 230.164,-69.6528 239.618,-60.3497\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"242.321,-62.5995 246.994,-53.0908 237.412,-57.6101 242.321,-62.5995\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"246.793\" y=\"-74.39\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x4b30f28>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, the second round\n",
    "dot_data = tree.export_graphviz(bdt.estimators_[1], out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed class0 weights: 0.357142857143\n",
      "Summed class1 weights: 0.642857142857\n"
     ]
    }
   ],
   "source": [
    "# First Second Tree use X0<= 3.7253 as split criteria.\n",
    "# Look at root weight value\n",
    "print 'Summed class0 weights:',weight1[y==0].sum()\n",
    "print 'Summed class1 weights:',weight1[y==1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd round root gini is: 0.45915918\n"
     ]
    }
   ],
   "source": [
    "# Root gini can be calculated:\n",
    "print '2nd round root gini is:', 1-0.3571**2-0.6429**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below observations are belong to the left subnode:\n",
      "(array([0, 1, 2, 3, 8, 9], dtype=int64),)\n",
      "\n",
      "The classfication of these observations are:\n",
      "[0 1 0 1 1 1]\n",
      "Weight sum for class0 is: 0.142857142857\n",
      "Weight sum for class1 is: 0.571428571429\n"
     ]
    }
   ],
   "source": [
    "# Look at left subnode\n",
    "print 'The below observations are belong to the left subnode:'\n",
    "print np.where(X[:,0]<=3.7253)\n",
    "print\n",
    "print 'The classfication of these observations are:'\n",
    "print y[X[:,0]<=3.7253]\n",
    "# As we can see below, there are 6 observations in this subnode; There are 2 observations are with class 0 and\n",
    "# 4 observations are with class 1 so we can calculate the weight value\n",
    "print 'Weight sum for class0 is:', weight1[(X[:,0]<=3.7253)&(y==0)].sum()\n",
    "print 'Weight sum for class1 is:', weight1[(X[:,0]<=3.7253)&(y==1)].sum()\n",
    "# Subnode Gini can be calculated with nomalized weight sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below observations are belong to the right subnode:\n",
      "(array([4, 5, 6, 7], dtype=int64),)\n",
      "\n",
      "The classfication of these observations are:\n",
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Look at Right subnode\n",
    "print 'The below observations are belong to the right subnode:'\n",
    "print np.where(X[:,0]>3.7253)\n",
    "print\n",
    "print 'The classfication of these observations are:'\n",
    "print y[X[:,0]>3.7253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the misclassified observations are:\n",
      "[0, 2, 4]\n",
      "the weight of these observations are:\n",
      "[ 0.07142857  0.07142857  0.07142857]\n",
      "The error term err1 is: 0.214285714286\n",
      "\n",
      "The alpha term alpla1 is: 1.29928298413\n"
     ]
    }
   ],
   "source": [
    "# the error term of the second decision tree is\n",
    "print 'the misclassified observations are:'\n",
    "print [0,2,4]\n",
    "print 'the weight of these observations are:'\n",
    "print weight1[[0,2,4]]\n",
    "misclassified = weight1[[0,2,4]]\n",
    "err1 = misclassified.sum()/weight1.sum()\n",
    "print 'The error term err1 is:', err1\n",
    "print\n",
    "alpha1 = np.log((1-err1)/err1)\n",
    "print 'The alpha term alpla1 is:',alpha1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learner weight of second Decision Tree is: 1.29928298413\n",
      "The error of second Decision Tree is: 0.214285714286\n"
     ]
    }
   ],
   "source": [
    "# We can also cross verify the alpha1 value using method estimators_weight_\n",
    "print 'The learner weight of second Decision Tree is:', bdt.estimator_weights_[1]\n",
    "print 'The error of second Decision Tree is:', bdt.estimator_errors_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample weight after second round is:\n",
      "[ 0.16666667  0.10606061  0.16666667  0.10606061  0.16666667  0.04545455\n",
      "  0.04545455  0.04545455  0.10606061  0.04545455]\n"
     ]
    }
   ],
   "source": [
    "# Now We need to update and renomalize the weight of all the observations, \n",
    "# Misclassified observations will be boosted in weights. \n",
    "print 'The sample weight after second round is:'\n",
    "weight2 = np.copy(weight1)\n",
    "weight2[[0,2,4]]=weight1[0]*np.exp(alpha1)\n",
    "weight2 = weight2/weight2.sum()\n",
    "print weight2\n",
    "# It can be seen that observation 0,2,4 has been boosted in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second stage decision function is:\n",
      "[ 0.21056049  0.21056049  0.21056049  0.21056049 -0.21056049 -1.         -1.\n",
      " -1.          0.21056049  1.        ]\n",
      "The second predict is:\n",
      "[1 1 1 1 0 0 0 0 1 1]\n",
      "The second predict probability is:\n",
      "[[ 0.43426073  0.56573927]\n",
      " [ 0.43426073  0.56573927]\n",
      " [ 0.43426073  0.56573927]\n",
      " [ 0.43426073  0.56573927]\n",
      " [ 0.47699634  0.52300366]\n",
      " [ 0.59900478  0.40099522]\n",
      " [ 0.59900478  0.40099522]\n",
      " [ 0.59900478  0.40099522]\n",
      " [ 0.43426073  0.56573927]\n",
      " [ 0.319105    0.680895  ]]\n"
     ]
    }
   ],
   "source": [
    "# The second stage decision function\n",
    "print 'The second stage decision function is:'\n",
    "for idx,item in enumerate(bdt.staged_decision_function(X)):\n",
    "    if idx==1:\n",
    "        print item\n",
    "        break\n",
    "# The second predict\n",
    "print 'The second predict is:'\n",
    "for idx,item in enumerate(bdt.staged_predict(X)):\n",
    "    if idx==1:\n",
    "        print item\n",
    "        predict1=item\n",
    "        break\n",
    "# The second predict probability\n",
    "print 'The second predict probability is:'\n",
    "for idx,item in enumerate(bdt.staged_predict_proba(X)):\n",
    "    if idx==1:\n",
    "        print item\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict0[predict0==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict1[predict1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"222pt\" height=\"161pt\"\r\n",
       " viewBox=\"0.00 0.00 222.00 161.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 157)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-157 218,-157 218,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147,-153C147,-153 68,-153 68,-153 62,-153 56,-147 56,-141 56,-141 56,-101 56,-101 56,-95 62,-89 68,-89 68,-89 147,-89 147,-89 153,-89 159,-95 159,-101 159,-101 159,-141 159,-141 159,-147 153,-153 147,-153\"/>\r\n",
       "<text text-anchor=\"start\" x=\"68.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\r\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\r\n",
       "<text text-anchor=\"start\" x=\"85.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 2.1592</text>\r\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"64\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"start\" x=\"67\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 5]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.400000\" stroke=\"black\" d=\"M87,-53C87,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 87,-0 87,-0 93,-0 99,-6 99,-12 99,-12 99,-41 99,-41 99,-47 93,-53 87,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4688</text>\r\n",
       "<text text-anchor=\"start\" x=\"10\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\r\n",
       "<text text-anchor=\"start\" x=\"9\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 3]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M88.0255,-88.9415C82.5461,-80.2028 76.5563,-70.6501 70.9675,-61.737\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"73.8238,-59.7038 65.5461,-53.0908 67.8932,-63.4224 73.8238,-59.7038\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"59.9601\" y=\"-73.7588\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M202,-53C202,-53 129,-53 129,-53 123,-53 117,-47 117,-41 117,-41 117,-12 117,-12 117,-6 123,-0 129,-0 129,-0 202,-0 202,-0 208,-0 214,-6 214,-12 214,-12 214,-41 214,-41 214,-47 208,-53 202,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"136.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"126\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\r\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.975,-88.9415C132.454,-80.2028 138.444,-70.6501 144.033,-61.737\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"147.107,-63.4224 149.454,-53.0908 141.176,-59.7038 147.107,-63.4224\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"155.04\" y=\"-73.7588\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x4bdf7f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.        ,  0.84729786],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.        ,  0.84729786]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X)*alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50473227  0.49526773]\n",
      " [ 0.37637722  0.62362278]\n",
      " [ 0.50932407  0.49067593]\n",
      " [ 0.37637722  0.62362278]\n",
      " [ 0.43413744  0.56586256]\n",
      " [ 0.63462844  0.36537156]\n",
      " [ 0.53039748  0.46960252]\n",
      " [ 0.63462844  0.36537156]\n",
      " [ 0.40050132  0.59949868]\n",
      " [ 0.30424128  0.69575872]]\n",
      "\n",
      "[[ 1.65654196  1.6409375 ]\n",
      " [ 1.45699664  1.86567474]\n",
      " [ 1.66416595  1.63341992]\n",
      " [ 1.45699664  1.86567474]\n",
      " [ 1.54363102  1.76096606]\n",
      " [ 1.88632112  1.44104935]\n",
      " [ 1.69960773  1.59935836]\n",
      " [ 1.88632112  1.44104935]\n",
      " [ 1.49257276  1.82120557]\n",
      " [ 1.3555961   2.0052299 ]]\n",
      "\n",
      "[[ 3.29747946]\n",
      " [ 3.32267138]\n",
      " [ 3.29758588]\n",
      " [ 3.32267138]\n",
      " [ 3.30459708]\n",
      " [ 3.32737047]\n",
      " [ 3.29896609]\n",
      " [ 3.32737047]\n",
      " [ 3.31377833]\n",
      " [ 3.360826  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_classes=bdt.n_classes_\n",
    "proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(bdt.estimators_,\n",
    "                                                bdt.estimator_weights_))\n",
    "\n",
    "proba /= bdt.estimator_weights_.sum()\n",
    "print proba\n",
    "print\n",
    "proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "print proba\n",
    "print\n",
    "normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "print normalizer\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.29747946,  3.32267138,  3.29758588,  3.32267138,  3.30459708,\n",
       "        3.32737047,  3.29896609,  3.32737047,  3.31377833,  3.360826  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50236612,  0.49763388],\n",
       "       [ 0.43850158,  0.56149842],\n",
       "       [ 0.5046619 ,  0.4953381 ],\n",
       "       [ 0.43850158,  0.56149842],\n",
       "       [ 0.46711626,  0.53288374],\n",
       "       [ 0.56691046,  0.43308954],\n",
       "       [ 0.51519406,  0.48480594],\n",
       "       [ 0.56691046,  0.43308954],\n",
       "       [ 0.45041418,  0.54958582],\n",
       "       [ 0.40335206,  0.59664794]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer[normalizer == 0.0] = 1.0\n",
    "proba /= normalizer\n",
    "print proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = bdt.estimator_weights_[0]\n",
    "proba = bdt.estimators_[0].predict_proba(X) * norm\n",
    "real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))\n",
    "normalizer = real_proba.sum(axis=1)[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizer[normalizer == 0.0] = 1.0\n",
    "real_proba /= normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.        ,  0.84729786],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.52956116,  0.3177367 ],\n",
       "       [ 0.        ,  0.84729786]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.estimators_[0].predict_proba(X)*norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.estimators_[0].predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = (bdt.estimators_[0].predict(X)==bdt.classes_[:,np.newaxis]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.classes_[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = temp*alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84729786,  0.        ],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.        ,  0.84729786],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.84729786,  0.        ],\n",
       "       [ 0.        ,  0.84729786]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred[:,0]*=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.84729786,  0.        ],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.        ,  0.84729786],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.84729786,  0.        ],\n",
       "       [-0.        ,  0.84729786]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "        norm = 0.\n",
    "\n",
    "        for weight, estimator in zip(self.estimator_weights_,\n",
    "                                     self.estimators_):\n",
    "            norm += weight\n",
    "\n",
    "            if self.algorithm == 'SAMME.R':\n",
    "                # The weights are all 1. for SAMME.R\n",
    "                current_pred = _samme_proba(estimator, n_classes, X)\n",
    "            else:  # elif self.algorithm == \"SAMME\":\n",
    "                current_pred = estimator.predict(X)\n",
    "                current_pred = (current_pred == classes).T * weight\n",
    "\n",
    "            if pred is None:\n",
    "                pred = current_pred\n",
    "            else:\n",
    "                pred += current_pred\n",
    "\n",
    "            if n_classes == 2:\n",
    "                tmp_pred = np.copy(pred)\n",
    "                tmp_pred[:, 0] *= -1\n",
    "                yield (tmp_pred / norm).sum(axis=1)\n",
    "            else:\n",
    "                yield pred / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current norm is:  0.847297860387\n",
      "\n",
      "current pred is:  [0 0 0 0 1 0 0 0 0 1]\n",
      "weighted pred is:  [[ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.          0.84729786]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.          0.84729786]]\n",
      "summed pred is:  [[ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.          0.84729786]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.84729786  0.        ]\n",
      " [ 0.          0.84729786]]\n",
      "[-1. -1. -1. -1.  1. -1. -1. -1. -1.  1.]\n",
      "current norm is:  2.14658084452\n",
      "\n",
      "current pred is:  [1 1 1 1 0 0 0 0 1 1]\n",
      "weighted pred is:  [[ 0.          1.29928298]\n",
      " [ 0.          1.29928298]\n",
      " [ 0.          1.29928298]\n",
      " [ 0.          1.29928298]\n",
      " [ 1.29928298  0.        ]\n",
      " [ 1.29928298  0.        ]\n",
      " [ 1.29928298  0.        ]\n",
      " [ 1.29928298  0.        ]\n",
      " [ 0.          1.29928298]\n",
      " [ 0.          1.29928298]]\n",
      "summed pred is:  [[ 0.84729786  1.29928298]\n",
      " [ 0.84729786  1.29928298]\n",
      " [ 0.84729786  1.29928298]\n",
      " [ 0.84729786  1.29928298]\n",
      " [ 1.29928298  0.84729786]\n",
      " [ 2.14658084  0.        ]\n",
      " [ 2.14658084  0.        ]\n",
      " [ 2.14658084  0.        ]\n",
      " [ 0.84729786  1.29928298]\n",
      " [ 0.          2.14658084]]\n",
      "[ 0.21056049  0.21056049  0.21056049  0.21056049 -0.21056049 -1.         -1.\n",
      " -1.          0.21056049  1.        ]\n",
      "current norm is:  3.20518779857\n",
      "\n",
      "current pred is:  [0 0 0 0 1 0 1 0 1 1]\n",
      "weighted pred is:  [[ 1.05860695  0.        ]\n",
      " [ 1.05860695  0.        ]\n",
      " [ 1.05860695  0.        ]\n",
      " [ 1.05860695  0.        ]\n",
      " [ 0.          1.05860695]\n",
      " [ 1.05860695  0.        ]\n",
      " [ 0.          1.05860695]\n",
      " [ 1.05860695  0.        ]\n",
      " [ 0.          1.05860695]\n",
      " [ 0.          1.05860695]]\n",
      "summed pred is:  [[ 1.90590481  1.29928298]\n",
      " [ 1.90590481  1.29928298]\n",
      " [ 1.90590481  1.29928298]\n",
      " [ 1.90590481  1.29928298]\n",
      " [ 1.29928298  1.90590481]\n",
      " [ 3.2051878   0.        ]\n",
      " [ 2.14658084  1.05860695]\n",
      " [ 3.2051878   0.        ]\n",
      " [ 0.84729786  2.35788994]\n",
      " [ 0.          3.2051878 ]]\n",
      "[-0.18926249 -0.18926249 -0.18926249 -0.18926249  0.18926249 -1.\n",
      " -0.33944154 -1.          0.47129597  1.        ]\n",
      "current norm is:  4.24231500359\n",
      "\n",
      "current pred is:  [1 1 0 1 1 1 1 1 1 1]\n",
      "weighted pred is:  [[ 0.          1.03712721]\n",
      " [ 0.          1.03712721]\n",
      " [ 1.03712721  0.        ]\n",
      " [ 0.          1.03712721]\n",
      " [ 0.          1.03712721]\n",
      " [ 0.          1.03712721]\n",
      " [ 0.          1.03712721]\n",
      " [ 0.          1.03712721]\n",
      " [ 0.          1.03712721]\n",
      " [ 0.          1.03712721]]\n",
      "summed pred is:  [[ 1.90590481  2.33641019]\n",
      " [ 1.90590481  2.33641019]\n",
      " [ 2.94303202  1.29928298]\n",
      " [ 1.90590481  2.33641019]\n",
      " [ 1.29928298  2.94303202]\n",
      " [ 3.2051878   1.03712721]\n",
      " [ 2.14658084  2.09573416]\n",
      " [ 3.2051878   1.03712721]\n",
      " [ 0.84729786  3.39501714]\n",
      " [ 0.          4.242315  ]]\n",
      "[ 0.10147888  0.10147888 -0.38746511  0.10147888  0.38746511 -0.51105601\n",
      " -0.0119856  -0.51105601  0.60054929  1.        ]\n",
      "current norm is:  5.50136071526\n",
      "\n",
      "current pred is:  [0 1 1 1 0 0 0 0 0 0]\n",
      "weighted pred is:  [[ 1.25904571  0.        ]\n",
      " [ 0.          1.25904571]\n",
      " [ 0.          1.25904571]\n",
      " [ 0.          1.25904571]\n",
      " [ 1.25904571  0.        ]\n",
      " [ 1.25904571  0.        ]\n",
      " [ 1.25904571  0.        ]\n",
      " [ 1.25904571  0.        ]\n",
      " [ 1.25904571  0.        ]\n",
      " [ 1.25904571  0.        ]]\n",
      "summed pred is:  [[ 3.16495053  2.33641019]\n",
      " [ 1.90590481  3.5954559 ]\n",
      " [ 2.94303202  2.5583287 ]\n",
      " [ 1.90590481  3.5954559 ]\n",
      " [ 2.5583287   2.94303202]\n",
      " [ 4.46423351  1.03712721]\n",
      " [ 3.40562656  2.09573416]\n",
      " [ 4.46423351  1.03712721]\n",
      " [ 2.10634357  3.39501714]\n",
      " [ 1.25904571  4.242315  ]]\n",
      "[-0.15060644  0.30711513 -0.06992876  0.30711513  0.06992876 -0.62295612\n",
      " -0.23810335 -0.62295612  0.23424633  0.54227844]\n"
     ]
    }
   ],
   "source": [
    "classes =  bdt.classes_[:,np.newaxis]\n",
    "pred = None\n",
    "norm = 0\n",
    "\n",
    "for weight, estimator in zip(bdt.estimator_weights_,bdt.estimators_):\n",
    "    norm += weight\n",
    "    print 'current norm is: ',norm\n",
    "    print\n",
    "    current_pred = estimator.predict(X)\n",
    "    print 'current pred is: ',current_pred\n",
    "    current_pred = (current_pred == classes).T * weight\n",
    "    print 'weighted pred is: ', current_pred\n",
    "    \n",
    "    if pred is None:\n",
    "        pred = current_pred\n",
    "    else:\n",
    "        pred += current_pred\n",
    "    print 'summed pred is: ', pred\n",
    "    \n",
    "    if n_classes==2:\n",
    "        tmp_pred = np.copy(pred)\n",
    "        tmp_pred[:,0] *= -1\n",
    "        print (tmp_pred/norm).sum(axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
